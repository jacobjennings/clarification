{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b317b75efd19d842",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:33:43.256489Z",
     "start_time": "2024-11-27T22:33:42.629596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import logging\n",
    "import secrets\n",
    "import numpy\n",
    "import copy\n",
    "import gc\n",
    "import math\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "\n",
    "import time\n",
    "\n",
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from complexPyTorch.complexFunctions import complex_relu\n",
    "\n",
    "import auraloss\n",
    "\n",
    "# Audio\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from torio.io import CodecConfig\n",
    "\n",
    "# Image datasets and image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as TVM\n",
    "\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b02c0dbc4ad760",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a680190877767773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:42:07.271846Z",
     "start_time": "2024-04-27T22:42:07.269507Z"
    }
   },
   "outputs": [],
   "source": [
    "base_dataset_directory = '/home/jacob/cv-corpus-17.0-2024-03-15/en'\n",
    "noisy_dataset_directory = '/home/jacob/noisy-commonvoice/en'\n",
    "models_dir = '/home/jacob/denoise-models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcbe117b485c88",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c92d166f0bc10d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:42:13.525473Z",
     "start_time": "2024-04-27T22:42:11.329523Z"
    }
   },
   "outputs": [],
   "source": [
    "common_voice_dataset = torchaudio.datasets.COMMONVOICE(root=base_dataset_directory)\n",
    "common_voice_noisy_dataset = torchaudio.datasets.COMMONVOICE(root=noisy_dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfca0e0-033d-4da4-b362-63c99e0aa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, _ = waveform.shape\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "    figure.suptitle(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabf2f1564f979e",
   "metadata": {},
   "source": [
    "### Load datasets and create train / test splits. The same seed is used for splitting noisy and clear datasets so the files match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862c1774d190c8b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:46:31.938391Z",
     "start_time": "2024-04-27T22:44:38.020628Z"
    }
   },
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "\n",
    "clear_loader = DataLoader(\n",
    "    common_voice_dataset,\n",
    "    batch_size=1)\n",
    "\n",
    "noisy_loader = DataLoader(\n",
    "    common_voice_noisy_dataset,\n",
    "    batch_size=1)\n",
    "\n",
    "split_generator_0 = torch.Generator().manual_seed(314)\n",
    "noisy_train, noisy_test = random_split(noisy_loader.dataset, [0.9, 0.1], generator=split_generator_0)\n",
    "\n",
    "split_generator_1 = torch.Generator().manual_seed(314)\n",
    "clear_train, clear_test = random_split(clear_loader.dataset, [0.9, 0.1], generator=split_generator_1)\n",
    "\n",
    "# noisy_1 = next(iter(noisy_train))\n",
    "# clear_1 = next(iter(clear_train))\n",
    "\n",
    "# Audio(noisy_1[0].squeeze(), rate=48000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b18c9e-6909-4c0a-9b96-fdeda195eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(clear_1[0].squeeze(), rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08810366743b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "id": "004e423a-c8ab-4fc1-80bb-37e375d722f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:37:18.240044Z",
     "start_time": "2024-11-27T22:37:18.174042Z"
    }
   },
   "source": [
    "### Create a model\n",
    "\n",
    "sample_rate = 48000\n",
    "\n",
    "sample_batch_ms = 50\n",
    "hidden_size_ms = 200\n",
    "\n",
    "samples_per_batch = int((sample_batch_ms / 1000) * sample_rate)\n",
    "samples_per_hidden = int((hidden_size_ms / 1000) * sample_rate)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class ComplexRelu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexRelu, self).__init__()\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = complex_relu(x)\n",
    "        return x\n",
    "\n",
    "class UNet1dDown(nn.Module):\n",
    "    def __init__(self, device, dtype):\n",
    "        super(UNet1dDown, self).__init__()\n",
    "        \n",
    "        self.conv1_out_channels = 16\n",
    "\n",
    "        self.c1 = nn.Conv1d(in_channels=1, out_channels=self.conv1_out_channels, kernel_size=3, stride=1, padding=1, device=device, dtype=dtype)\n",
    "        self.c2 = nn.Conv1d(in_channels=self.conv1_out_channels, out_channels=32, kernel_size=3, stride=1, padding=1, device=device, dtype=dtype)\n",
    "        self.c3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=3, padding=1, device=device, dtype=dtype)\n",
    "        self.c4 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=3, padding=1, device=device, dtype=dtype)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "class UNet1d(nn.Module):\n",
    "    def __init__(self, device, dtype):\n",
    "        super(UNet1d, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DenoisingModel(nn.Module):\n",
    "    def __init__(self, device, dtype):\n",
    "        super(DenoisingModel, self).__init__()\n",
    "        \n",
    "        n_fft = 400\n",
    "        # self.spectrogram = T.Spectrogram(n_fft=n_fft, power=2, wkwargs={'device': device})\n",
    "        \n",
    "        self.melspec = T.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, power=2, wkwargs={'device': device})\n",
    "        self.griffin = T.GriffinLim(n_fft=n_fft, wkwargs={'device': device})\n",
    "        \n",
    "        self.downsampleNet = TVM.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "        self.inverseMel = T.InverseMelScale(n_stft=n_fft // 2 + 1, sample_rate=sample_rate)\n",
    "\n",
    "        # spectrogram_size = n_fft / 2 + 1\n",
    "        \n",
    "        # self.inverseSpectrogram = T.InverseSpectrogram(wkwargs = {'device': device})\n",
    "        # self.conv1_out_channels = 16\n",
    "\n",
    "        # self.c1 = nn.Conv2d(in_channels=1, out_channels=self.conv1_out_channels, kernel_size=3, stride=1, padding=1, device=device, dtype=dtype)\n",
    "\n",
    "        # self.c1 = nn.Conv2d(in_channels=1, out_channels=self.conv1_out_channels, kernel_size=3, stride=1, padding=1, device=device, dtype=dtype)\n",
    "        # self.c2 = nn.Conv2d(in_channels=self.conv1_out_channels, out_channels=32, kernel_size=3, stride=1, padding=1, device=device, dtype=dtype)\n",
    "        # self.c3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=3, padding=1, device=device, dtype=dtype)\n",
    "        # self.c4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=3, padding=1, device=device, dtype=dtype)\n",
    "\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        print(f\"samples_per_batch: {samples_per_batch}, samples_per_hidden: {samples_per_hidden}\")\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=201, hidden_size=201 * 5, num_layers=3, dropout=0.3, device=device, dtype=dtype)\n",
    "\n",
    "        # dense_size = 3000\n",
    "        \n",
    "        # self.fc1 = nn.Linear(in_features = 5 * 2613, out_features = 6000, device=device, dtype=dtype)\n",
    "        # self.fc2 = nn.Linear(in_features = 6000, out_features = 5000, device=device, dtype=dtype)\n",
    "        # self.fc3 = nn.Linear(in_features = 5000, out_features = 4000, device=device, dtype=dtype)\n",
    "        # self.fc4 = nn.Linear(in_features = 4000, out_features = 201 * 13, device=device, dtype=dtype)\n",
    "        \n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.softmax = nn.Softmax()\n",
    "        \n",
    "        # self.normy = nn.BatchNorm1d(65, device=device)\n",
    "\n",
    "        # self.fc1 = nn.Linear(in_features = 2048, out_features = 6000, device=device, dtype=dtype)\n",
    "        # self.fc2 = nn.Linear(in_features = 6000, out_features = 8000, device=device, dtype=dtype)\n",
    "        # self.fc3 = nn.Linear(in_features = 8000, out_features = 151 * 65, device=device, dtype=dtype)\n",
    "        # self.fc4 = nn.Linear(in_features = 151 * 65, out_features = 151 * 65, device=device, dtype=dtype)\n",
    "\n",
    "        \n",
    "        # self.transformer = nn.Transformer(\n",
    "        #     d_model=input_dim,\n",
    "        #     nhead=nhead,\n",
    "        #     num_encoder_layers=num_encoder_layers,\n",
    "        #     num_decoder_layers=num_decoder_layers,\n",
    "        #     dim_feedforward=dim_feedforward,\n",
    "        #     batch_first=True,\n",
    "        #     device=device,\n",
    "        #     dtype=dtype,\n",
    "        # )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.enable_grad():\n",
    "            x.requires_grad = True\n",
    "            x = x.to(device)\n",
    "\n",
    "            # print(f\"000 {x}\")\n",
    "\n",
    "            # print(f\"1 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")        \n",
    "            # spec = self.spectrogram(x).unsqueeze(0)\n",
    "            # spec = self.melspec(x).unsqueeze(0)\n",
    "            # spec = spec.permute(2, 0, 1)\n",
    "            \n",
    "            \n",
    "\n",
    "            # print(f\"x.max(): {x.max()}\")            \n",
    "            # print(f\"100 {x}\")\n",
    "            # print(f\"X MAX: {x.max()}\")\n",
    "            # x = x / 2048.0\n",
    "            # print(f\"X MAX after: {x.max()}\")\n",
    "            print(x)            \n",
    "            \n",
    "            self.downsampleNet(x)\n",
    "            \n",
    "            print(x)\n",
    "\n",
    "            # print(f\"2 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = x.unsqueeze(0)\n",
    "            # print(f\"3 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            \n",
    "            # x = self.c1(x)\n",
    "            # print(f\"4 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # \n",
    "            # x = torch.relu(x)\n",
    "            # \n",
    "            # print(f\"5 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            \n",
    "            # print(f\"6 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = self.c2(x)\n",
    "            # x = self.dropout(x)\n",
    "\n",
    "            # x = self.c3(x)\n",
    "            # x = self.dropout(x)\n",
    "            \n",
    "            # x = self.c4(x)\n",
    "            \n",
    "            # x, _ = self.lstm(x)\n",
    "            \n",
    "            # x = self.dropout(x)\n",
    "\n",
    "            # x = self.pool(x)\n",
    "\n",
    "            # print(f\"200 {x}\")\n",
    "\n",
    "            # print(f\"7 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            \n",
    "            # x = x.view(-1, 201 * 65)\n",
    "\n",
    "            # print(x)\n",
    "            \n",
    "            # print(f\"8 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = self.fc1(x)\n",
    "            # x = self.relu(x)\n",
    "            # x = self.dropout(x)\n",
    "\n",
    "            # print(f\"9 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = self.fc2(x)\n",
    "            # x = self.softmax(x)\n",
    "            # x = self.dropout(x)\n",
    "\n",
    "            # print(f\"10 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = self.fc3(x)\n",
    "            # x = self.dropout(x)\n",
    "\n",
    "            # print(f\"11 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = self.fc4(x)\n",
    "            # x = self.dropout(x)\n",
    "\n",
    "            # x = self.dropout(x)\n",
    "\n",
    "            # print(f\"300 {x}\")\n",
    "            \n",
    "            # print(f\"12 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = x.view(201, 13)\n",
    "\n",
    "            # x = self.normy(x)\n",
    "\n",
    "            # print(f\"400 {x}\")\n",
    "\n",
    "            # print(f\"13 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "\n",
    "            # x = x - x.min() + 0.001\n",
    "            # x = x / 2.0\n",
    "            # x = torch.clamp(x, min=1e-10)\n",
    "            # x = x * 100.0\n",
    "\n",
    "            # print(f\"x.max(): {x.max()}\")\n",
    "            # print(f\"x min: {x.min()}\")\n",
    "            # print(f\"500 {x}\") \n",
    "            # print(f\"x min: {x.min()}\")\n",
    "            # x = self.griffin(x)\n",
    "\n",
    "            # print(f\"600 {x}\")\n",
    "\n",
    "            # print(f\"9 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "\n",
    "        \n",
    "            # x = self.inverseSpectrogram(x)\n",
    "            # print(x)\n",
    "            # print(f\"9 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "            # x = x.to(device)\n",
    "            # print(f\"10 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")\n",
    "        return x        \n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    torch.cuda.empty_cache()\n",
    "    dtype=torch.float32\n",
    "    \n",
    "    # sequence_model = nn.Sequential(\n",
    "    #     nn.Linear(in_features=samples_per_batch, out_features=samples_per_batch, device=device, dtype=dtype),\n",
    "    #     nn.Linear(in_features=samples_per_batch, out_features=int(samples_per_batch * 0.8), device=device, dtype=dtype),\n",
    "    #     nn.Linear(in_features=int(samples_per_batch * 0.8), out_features=int(samples_per_batch * 0.6), device=device, dtype=dtype),\n",
    "    #     nn.Linear(in_features=int(samples_per_batch * 0.6), out_features=int(samples_per_batch * 0.4), device=device, dtype=dtype),\n",
    "    #     # ComplexRelu(),\n",
    "    #     # nn.ReLU(),\n",
    "    #     nn.Linear(in_features=int(samples_per_batch * 0.4), out_features=int(samples_per_batch * 0.6), device=device, dtype=dtype),    \n",
    "    #     nn.Linear(in_features=int(samples_per_batch * 0.6), out_features=int(samples_per_batch * 0.8), device=device, dtype=dtype),    \n",
    "    #     nn.Linear(in_features=int(samples_per_batch * 0.8), out_features=samples_per_batch, device=device, dtype=dtype),\n",
    "    #     nn.Linear(in_features=samples_per_batch, out_features=samples_per_batch, device=device, dtype=dtype),        \n",
    "    # )\n",
    "\n",
    "    # sequence_model = nn.Sequential(\n",
    "        # T.MuLawEncoding(quantization_channels=256),\n",
    "        # DenoisingModel(input_dim=1, nhead=1, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024, device=device, dtype=dtype),\n",
    "        # T.MuLawDecoding(quantization_channels=256),\n",
    "        \n",
    "        # nn.Linear(in_features=samples_per_batch, out_features=samples_per_batch, device=device, dtype=dtype),\n",
    "        # nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, device=device, dtype=dtype),\n",
    "        \n",
    "        \n",
    "        # nn.Linear(in_features=samples_per_batch, out_features=samples_per_batch, device=device, dtype=dtype),\n",
    "        # nn.Transformer(nhead=16, num_encoder_layers=12, num_decoder_layers=12, dim_feedforward=samples_per_batch, batch_first=True, device=device, dtype=dtype),\n",
    "        # nn.TransformerEncoderLayer(d_model=512, nhead=16, dim_feedforward=2048, dropout=0.1, device=device, dtype=dtype),\n",
    "        # nn.Linear(in_features=samples_per_batch, out_features=samples_per_batch, device=device, dtype=dtype),        \n",
    "    # )\n",
    "    \n",
    "    sequence_model = DenoisingModel(device=device, dtype=dtype)\n",
    "    \n",
    "    loss_fn = nn.L1Loss()\n",
    "    # loss_fn = auraloss.time.SNRLoss()\n",
    "    # loss_fn = auraloss.freq.SumAndDifferenceSTFTLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(params=sequence_model.parameters(), lr=0.01)\n",
    "\n",
    "    print(sequence_model)\n",
    "    \n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m samples_per_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m((sample_batch_ms \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1000\u001B[39m) \u001B[38;5;241m*\u001B[39m sample_rate)\n\u001B[1;32m      9\u001B[0m samples_per_hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m((hidden_size_ms \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1000\u001B[39m) \u001B[38;5;241m*\u001B[39m sample_rate)\n\u001B[0;32m---> 11\u001B[0m \u001B[43mgc\u001B[49m\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m     12\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mComplexRelu\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mModule):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gc' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7df1ff1b-b4ad-40cd-8f28-25f96a137175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e4c96e3-0eea-4897-a54b-8030dd3361e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:14:51.890339Z",
     "start_time": "2024-11-21T02:14:51.838712Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DenoisingModel' object has no attribute 'spectrogram'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:48\u001B[0m\n",
      "File \u001B[0;32m~/jupyter-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/jupyter-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[14], line 94\u001B[0m, in \u001B[0;36mDenoisingModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     89\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;66;03m# print(f\"000 {x}\")\u001B[39;00m\n\u001B[1;32m     92\u001B[0m \n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# print(f\"1 size of x: {x.size()} dtype: {x.dtype} has_nans: {torch.isnan(x).any()}\")        \u001B[39;00m\n\u001B[0;32m---> 94\u001B[0m spec \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspectrogram\u001B[49m(x)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     95\u001B[0m spec \u001B[38;5;241m=\u001B[39m spec\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# print(f\"x.max(): {x.max()}\")            \u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# print(f\"100 {x}\")\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# print(f\"X MAX: {x.max()}\")\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;66;03m# x = x / 2048.0\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# print(f\"X MAX after: {x.max()}\")\u001B[39;00m\n",
      "File \u001B[0;32m~/jupyter-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1708\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1709\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DenoisingModel' object has no attribute 'spectrogram'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Train\n",
    "\n",
    "files_processed = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "resample_rate = 48000\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    noisy_iter = iter(noisy_train)\n",
    "    clear_iter = iter(clear_train)\n",
    "    while time.time() - t0 < 6:\n",
    "        noisy_complete = next(noisy_iter, None)\n",
    "        if noisy_complete is None:\n",
    "            break\n",
    "        \n",
    "        noisy = noisy_complete[0].squeeze()\n",
    "        clear = next(clear_iter)[0].squeeze()\n",
    "\n",
    "        resampler = T.Resample(noisy_complete[1], resample_rate, dtype=torch.float32)\n",
    "        noisy = resampler(noisy)\n",
    "        clear = resampler(clear)\n",
    "\n",
    "        files_processed += 1\n",
    "        \n",
    "        noisy_split = torch.split(noisy, samples_per_batch)\n",
    "        clear_split = torch.split(clear, samples_per_batch)\n",
    "\n",
    "        loss_sum = 0\n",
    "        for split_idx, noisy_batch in enumerate(noisy_split):\n",
    "            # time1 = time.perf_counter()\n",
    "\n",
    "            noisy_batch = noisy_batch.to(device)\n",
    "            \n",
    "            noisy_pad = nn.ZeroPad1d((0, samples_per_batch - noisy_batch.size()[0]))\n",
    "            noisy_batch = noisy_pad(noisy_batch)\n",
    "            \n",
    "            clear_batch = clear_split[split_idx].to(device)\n",
    "            clear_pad = nn.ZeroPad1d((0, samples_per_batch - clear_batch.size()[0]))\n",
    "            clear_batch = clear_pad(clear_batch)\n",
    "\n",
    "            # time2 = time.perf_counter()\n",
    "            \n",
    "            sequence_model.train()\n",
    "\n",
    "            # time3 = time.perf_counter()\n",
    "            \n",
    "            prediction = sequence_model(noisy_batch)\n",
    "\n",
    "            # time4 = time.perf_counter()\n",
    "            \n",
    "            # prediction = torch.fft.ifft(sequence_model(torch.fft.fft(noisy_batch)))\n",
    "            loss = loss_fn(prediction, clear_batch)\n",
    "\n",
    "            if math.isnan(loss):\n",
    "                nan_in_prediction = \"Yes\" if torch.isnan(prediction).any() else \"No\"\n",
    "                print(f\"ERROR: NaN loss. NaN in prediction? {nan_in_prediction}\")\n",
    "                raise KeyboardInterrupt\n",
    "\n",
    "            # time5 = time.perf_counter()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # time6 = time.perf_counter()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # time7 = time.perf_counter()\n",
    "            # for name, param in sequence_model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         grad_mean = param.grad.mean().item()\n",
    "            #         grad_std = param.grad.std().item()\n",
    "            #         print(f\"{name}: grad_mean = {grad_mean}, grad_std = {grad_std}\")\n",
    "            #     else:\n",
    "            #         print(f\"No gradient for {name}\")\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            # time8 = time.perf_counter()\n",
    "            \n",
    "            sequence_model.eval()\n",
    "\n",
    "            # time9 = time.perf_counter()\n",
    "\n",
    "            loss_sum += loss\n",
    "        \n",
    "        if files_processed % 25 == 0:\n",
    "            elapsed_str = str(timedelta(seconds=time.time() - t0))\n",
    "            print(f\"Loss: {(loss_sum / 25.0):.12f}\\t elapsed: {elapsed_str}\\tfiles_processed: {files_processed}\")\n",
    "            loss_sum = 0\n",
    "            # 1-2: {(time2 - time1):.5f} 2-3: {(time3 - time2):.5f} 3-4: {(time4 - time3):.5f} 4-5: {(time5 - time4):.5f} 5-6: {(time6 - time5):.5f} 6-7: {(time7 - time6):.5f} 7-8: {(time8 - time7):.5f} 8-9: {(time9 - time8):.5f}\n",
    "\n",
    "    noisy_iter = iter(noisy_train)\n",
    "    clear_iter = iter(clear_train)\n",
    "\n",
    "    keep_going = True\n",
    "    while keep_going:\n",
    "        noisy_complete = next(noisy_iter)\n",
    "        noisy = noisy_complete[0].squeeze()\n",
    "        clear = next(clear_iter)[0].squeeze()\n",
    "        if noisy_complete[1] == 48000:\n",
    "            keep_going = False\n",
    "        else:\n",
    "            keep_going = True\n",
    "        \n",
    "    noisy_split = torch.split(noisy, samples_per_batch)\n",
    "    clear_split = torch.split(clear, samples_per_batch)\n",
    "\n",
    "    prediction_reconstructed = None\n",
    "    \n",
    "    for split_idx, noisy_batch in enumerate(noisy_split):\n",
    "        noisy_batch = noisy_batch.to(device)\n",
    "        \n",
    "        noisy_pad = nn.ZeroPad1d((0, samples_per_batch - noisy_batch.size()[0]))\n",
    "        noisy_batch = noisy_pad(noisy_batch)\n",
    "        \n",
    "        clear_batch = clear_split[split_idx].to(device)\n",
    "        clear_pad = nn.ZeroPad1d((0, samples_per_batch - clear_batch.size()[0]))\n",
    "        clear_batch = clear_pad(clear_batch)\n",
    "\n",
    "        # prediction = torch.fft.ifft(sequence_model(torch.fft.fft(noisy_batch)))\n",
    "        prediction = sequence_model(noisy_batch)\n",
    "        if prediction_reconstructed is not None:\n",
    "            prediction_reconstructed = torch.cat((prediction_reconstructed, prediction))\n",
    "        else:\n",
    "            prediction_reconstructed = prediction\n",
    "\n",
    "    print(noisy.size())\n",
    "    print(prediction_reconstructed.size())\n",
    "    print(clear.size())\n",
    "\n",
    "    print(prediction_reconstructed)\n",
    "\n",
    "    torch.save(sequence_model, models_dir + f\"/model-{time.strftime(\"%Y%m%d-%H%M%S\")}\")\n",
    "\n",
    "    display(Audio(noisy.cpu().detach(), rate=48000))\n",
    "    display(Audio(prediction_reconstructed.cpu().detach(), rate=48000))\n",
    "    display(Audio(clear.cpu().detach(), rate=48000))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca8bcf-9a45-4704-bd4c-f1e38de1ca1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539d6cf-6398-4e33-85ec-daa79c3113fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
